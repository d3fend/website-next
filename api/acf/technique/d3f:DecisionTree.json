{"def_to_off":{"head":{"vars":["query_def_tech_label","top_def_tech_label","def_tactic_label","def_tactic_rel_label","def_tech_label","def_artifact_rel_label","def_artifact_label","off_artifact_label","off_artifact_rel_label","off_tech_label","off_tech_id","off_tech_parent_label","off_tech_parent_is_toplevel","off_tactic_rel_label","off_tactic_label","def_tactic","def_tactic_rel","tactic_def_tech","def_tech","def_artifact_rel","def_artifact","off_artifact","off_artifact_rel","off_tech","off_tech_parent","off_tactic_rel","off_tactic"]},"results":{"bindings":[]}},"description":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","owl":"http://www.w3.org/2002/07/owl#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#","skos":"http://www.w3.org/2004/02/skos/core#"},"@graph":[{"@id":"d3f:DecisionTree","@type":["owl:NamedIndividual","owl:Class"],"d3f:d3fend-id":"D3A-DT","d3f:definition":"Decision tree learning is a supervised learning approach used in statistics, data mining, and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.","d3f:kb-article":"## How it works\nA decision tree starts with a root node, which does not have any incoming branches. The outgoing branches from the root node then feed into the internal nodes, also known as decision nodes. Based on the available features, both node types conduct evaluations to form homogenous subsets, which are denoted by leaf nodes, or terminal nodes. The leaf nodes represent all the possible outcomes within the dataset.\n\n## Considerations\n\nWhile the basic underlying model is that of a decision tree, the decision tree node criteria, and the method for identifying splits varies significantly depending on the learning algorithm selected (e.g., CART, ID3, C4.5, C5.0, CHAID, MARS.)  Extensions like linear and logistic trees can add additional expressiveness as well.\n\n### Verification Approach\n- Use software libraries and tools built for ML where possible, so that the underlying code is verified by prior use.\n- Use standard ML measures for both verification (developmental test) and operational test stages (e.g., accuracy, precision, recall, F1, ROC, confusion matrices.)\n- Assay your data to determine if a decision tree learning method will be effective, and if additional steps or supplemental ML techniques should be incorporated into the solution:\n  - Missing values,\n  - Noise and/or error rates in input data for each feature, or\n  - Decision trees are ideal when decision boundaries can be found that lie along axes of features.\n  - Target class balance; decision tree learning algorithms are especially sensitive to unbalanced target classes.\n- Decision tree learning overfitting may require tuning algorithm hyperparameters such as tree depth, max features used, max leaf nodes, etc.\n- Pruning may result in a more robust model in real-word applications.\n\n### Validation Approach\n- Use operationally relevant data across the range of application's operating environment.\n  - Use a variety of data sets reflecting different operating and environment conditions.\n  - Apply cross-fold validation to see sensitivity to data variation and avoid overfitting operational models.\n- Use SMEs to investigate model errors for problem domain areas or conditions for which the model may underperform and suggest refinements.\n- Incorporate some kind of continuous validation to address concept drift and the need to retrain the model and/or check data quality.\n\n## References\n1. Decision tree learning. (2023, May 30). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Decision_tree_learning)\n2. Decision Trees. (n.d.). In _scikit-learn User Guide 1.2.2_. [Link](https://scikit-learn.org/stable/modules/tree.html)\n3. Concept drift. (2023, April 17). In _Wikipedia_. [Link](https://en.wikipedia.org/wiki/Concept_drift)\n4. 8 Concept Drift Detection Methods. (n.d.). In _Aporia Learning Center_. [Link](https://www.aporia.com/learn/data-drift/concept-drift-detection-methods/)","rdfs:label":"Decision Tree","rdfs:subClassOf":{"@id":"d3f:Classification"}}]},"digital_artifacts":{"head":{"vars":["query_def_tech_label","top_def_tech_label","def_tech_label","def_artifact_rel_label","def_artifact_label","def_tactic","def_tactic_rel","def_tech","def_artifact_rel","def_artifact"]},"results":{"bindings":[]}},"subtechniques":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","owl":"http://www.w3.org/2002/07/owl#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#","skos":"http://www.w3.org/2004/02/skos/core#"},"@graph":[]},"related_offensive_matrix":{},"references":{"@context":{"rdfs":"http://www.w3.org/2000/01/rdf-schema#","d3f":"http://d3fend.mitre.org/ontologies/d3fend.owl#"},"@graph":[]},"references_meta":{}}